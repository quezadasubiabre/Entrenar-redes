{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Como_entrenar_un_red_y_no_morir_en_el_intento.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuGvXuAkBsh9",
        "colab_type": "text"
      },
      "source": [
        "# Como entrenar una red en google colab y no morir en el intento\n",
        "\n",
        "Entrenar una red neuronal si no se tienen los recursos computacionales sufientes es una tarea imposible.\n",
        "\n",
        "Por suerte Google nos viene a ayudar con Google Colab, que nos facilita el uso de GPUs para entrenar redes neuronales.  Pero esto tiene ciertas limitaciones en tiempo de entrenamiento, por lo que cada cierto tiempo se cae el entorno de ejecución y se detiene el entrenamiento.\n",
        "\n",
        "En el presente documento, se desarrolla una métodologia para evitar perder los datos de entrenamiento e iniciar posteriormende desde donde habiamos quedado. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjWF3_nKEK1-",
        "colab_type": "code",
        "outputId": "88d6afc9-426b-4035-fe36-44fb8c862783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnz4d-bgLWgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c462ac9-0b47-4999-cebf-08235a9c3426"
      },
      "source": [
        "import os\n",
        "root = '/content/drive/My Drive/CNN'\n",
        "save_dir = os.path.join(root, 'weights')\n",
        "print(save_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CNN/weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8vt9VyrIkx3",
        "colab_type": "code",
        "outputId": "3d1479e2-c59e-439f-a62c-a854af3c0d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10 \n",
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4-JCJ8ZEPIz",
        "colab_type": "code",
        "outputId": "f35d949e-925f-495b-a696-8a07db398b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "n_classes = 10 \n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Modelo\n",
        "\n",
        "def conv2d(x, W, padding):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=padding)\n",
        "\n",
        "def maxpool2d(x, padding = 'VALID'):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=padding)\n",
        "\n",
        "# red ejemplo de https://keras.io/examples/cifar10_cnn/\n",
        "\n",
        "\n",
        "# Entrada grafo\n",
        "x = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
        "# salida grafo\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "# dropout \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cnn(x):\n",
        "    \n",
        "    weights = {\n",
        "    # 3 x 3 convolution, 1 input image, 32 outputsMax-Pool\n",
        "    'W_conv1': tf.Variable(tf.keras.initializers.glorot_uniform()([3, 3, 3, 32]), name='W_conv1'),\n",
        "    # 3 x 3 conv, 32 inputs, 32 outputs \n",
        "    'W_conv2': tf.Variable(tf.keras.initializers.glorot_uniform()([3, 3, 32, 32]), name='W_conv2'),\n",
        "\n",
        "    # 3 x 3 conv, 32 inputs, 64 outputs \n",
        "    'W_conv3': tf.Variable(tf.keras.initializers.glorot_uniform()([3, 3, 32, 64]), name='W_conv3'),\n",
        "    # 3 x 3 conv, 64 inputs, 64 outputs \n",
        "    'W_conv4': tf.Variable(tf.keras.initializers.glorot_uniform()([3, 3, 64, 64]), name='W_conv4'),\n",
        "\n",
        "    #clasificador    \n",
        "    # fully connected, 6*6*64 inputs, 512 outputs\n",
        "    'W_fc': tf.Variable(tf.keras.initializers.glorot_uniform()([6*6*64, 512]), name='W_fc'),\n",
        "    # 512 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.keras.initializers.glorot_uniform()([512, n_classes]), name='W_out')\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "    'b_conv1': tf.Variable(tf.zeros([32]), name='b_conv1'),\n",
        "    'b_conv2': tf.Variable(tf.zeros([32]), name='b_conv2'),\n",
        "    'b_conv3': tf.Variable(tf.zeros([64]), name='b_conv3'),\n",
        "    'b_conv4': tf.Variable(tf.zeros([64]), name='b_conv4'),\n",
        "\n",
        "    'b_fc': tf.Variable(tf.zeros([512]), name='b_fc'),\n",
        "    'out': tf.Variable(tf.zeros([n_classes]), name='b_out')\n",
        "    }\n",
        "    \n",
        "        \n",
        "    # convolutional layer 1\n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        conv1 = tf.nn.relu(conv2d(x, weights['W_conv1'], padding='SAME') + biases['b_conv1'])\n",
        "    \n",
        "    # convolutional layer 2\n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2'],padding = 'VALID') + biases['b_conv2'])\n",
        "        # Max Pooling (down-sampling)\n",
        "        conv2 = maxpool2d(conv2)\n",
        "\n",
        "  \n",
        "    \n",
        "    # convolutional layer 3\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        conv3 = tf.nn.relu(conv2d(conv2, weights['W_conv3'],padding = 'SAME') + biases['b_conv3'])\n",
        "    # convolutional layer 4architecture\n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        conv4 = tf.nn.relu(conv2d(conv3, weights['W_conv4'],padding = 'VALID') + biases['b_conv4'])\n",
        "        # max pooling\n",
        "        conv4 = maxpool2d(conv4)\n",
        "\n",
        "    \n",
        "    #clasificador\n",
        "    with tf.name_scope('capa_oculta') as scope:\n",
        "        # flatten\n",
        "        fc = tf.reshape(conv4, [-1, 6*6*64], name = 'flatten')\n",
        "        fc = tf.nn.relu(tf.matmul(fc, weights['W_fc']) + biases['b_fc'])\n",
        "    with tf.name_scope('capa_salida') as scope:\n",
        "        output = tf.matmul(fc, weights['out']) + biases['out']\n",
        "        \n",
        "    return output\n",
        "    \n",
        "\n",
        "\n",
        "def crear_batch(x_train, y_train, i, batch_size):\n",
        "    x_batch = x_train[i*batch_size: (i+1)*batch_size]\n",
        "    y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
        "    return x_batch, y_batch\n",
        "\n",
        "def train_cnn(x,y,epochs,x_train,y_train,batch_size, checkpoint=None, epoch_i=0):\n",
        "    output = cnn(x)\n",
        "    y_ = tf.nn.softmax(output, name='prediction')\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y), name='loss')\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(cost)\n",
        "    correct = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))  \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name= 'accuracy')\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # setup the initialisation operator\n",
        "    init_op = tf.global_variables_initializer()\n",
        "\n",
        "    vector_costos_train = np.zeros([epochs])\n",
        "    vector_costos_test = np.zeros([epochs])\n",
        "    vector_acc_test = np.zeros([epochs])\n",
        "    vector_acc_train = np.zeros([epochs])\n",
        "\n",
        "    last_checkpoint_dir = os.path.join(save_dir, 'last_checkpoint.ckpt')\n",
        "    best_checkpoint_dir = os.path.join(save_dir, 'best_checkpoint.ckpt')\n",
        "\n",
        "    \n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        if checkpoint == None:\n",
        "          sess.run(init_op) # cuando se realiza inicializacion aleatoria\n",
        "        else:\n",
        "          saver.restore(sess, checkpoint) # restaurar desde checkpoint\n",
        "\n",
        "        best_epoch_loss = 10000\n",
        "\n",
        "\n",
        "        \n",
        "        for epoch in range(epoch_i,epochs):\n",
        "            epoch_loss = 0\n",
        "            acc_train = 0\n",
        "            for i in range(int(len(x_train)/batch_size)):\n",
        "                x_batch, y_batch = crear_batch(x_train,y_train,i,batch_size)\n",
        "                _, c = sess.run([optimizer, cost], feed_dict={x: x_batch, y: y_batch})\n",
        "                acc_batch = accuracy.eval({x:x_batch, y:y_batch})\n",
        "                epoch_loss += c\n",
        "                acc_train += acc_batch\n",
        "            \n",
        "            print('Epoch', epoch, 'completed out of',epochs,'loss:',epoch_loss/(int(len(x_train)/batch_size)),\n",
        "                 'Loss test:',cost.eval({x:x_test, y:y_test}),\n",
        "                  'Accuracy train:', acc_train/(int(len(x_train)/batch_size)),\n",
        "                  'Accuracy test:',accuracy.eval({x:x_test, y:y_test})\n",
        "                 )\n",
        "            \n",
        " \n",
        "            \n",
        "            vector_costos_train[epoch] = epoch_loss/(int(len(x_train)/batch_size))\n",
        "            vector_costos_test[epoch] = cost.eval({x:x_test, y:y_test})\n",
        "            vector_acc_train[epoch] = acc_train/(int(len(x_train)/batch_size)) \n",
        "            vector_acc_test[epoch] = accuracy.eval({x:x_test, y:y_test})\n",
        "            \n",
        "           \n",
        "            if epoch%10 == 0:              \n",
        "              saver.save(sess,last_checkpoint_dir)\n",
        "\n",
        "\n",
        "            if (epoch_loss < best_epoch_loss):\n",
        "              best_epoch_loss = epoch_loss\n",
        "              best_accuracy = accuracy.eval({x:x_test, y:y_test})\n",
        "              save_path = saver.save(sess, best_checkpoint_dir)\n",
        "\n",
        "        print('El mejor accuracy en test es:', best_accuracy)\n",
        "      \n",
        "        return vector_costos_train, vector_costos_test, vector_acc_test, vector_acc_train\n",
        "        \n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJsZSnlItF1",
        "colab_type": "code",
        "outputId": "7dcacddd-f129-427b-efff-f711392baa36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "epochs = 20\n",
        "vector_costos_train, vector_costos_test, vector_acc_test, vector_acc_train = train_cnn(x,y,epochs,x_train,y_train,batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-a5e5bb8ded3c>:119: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch 0 completed out of 20 loss: 2.300763751298953 Loss test: 2.297692 Accuracy train: 0.10989583333333333 Accuracy test: 0.1235\n",
            "Epoch 1 completed out of 20 loss: 2.2947549135257037 Loss test: 2.2917926 Accuracy train: 0.14783653846153846 Accuracy test: 0.1626\n",
            "Epoch 2 completed out of 20 loss: 2.2881936843578634 Loss test: 2.2841907 Accuracy train: 0.16316105769230768 Accuracy test: 0.1604\n",
            "Epoch 3 completed out of 20 loss: 2.2787468451720017 Loss test: 2.2724652 Accuracy train: 0.17091346153846154 Accuracy test: 0.1715\n",
            "Epoch 4 completed out of 20 loss: 2.2633693285477468 Loss test: 2.252627 Accuracy train: 0.18387419871794872 Accuracy test: 0.1875\n",
            "Epoch 5 completed out of 20 loss: 2.236096830245776 Loss test: 2.2175226 Accuracy train: 0.19763621794871794 Accuracy test: 0.1977\n",
            "Epoch 6 completed out of 20 loss: 2.192596477117294 Loss test: 2.1699266 Accuracy train: 0.20833333333333334 Accuracy test: 0.2084\n",
            "Epoch 7 completed out of 20 loss: 2.144182880719503 Loss test: 2.1231036 Accuracy train: 0.22774439102564104 Accuracy test: 0.2278\n",
            "Epoch 8 completed out of 20 loss: 2.0945708941190673 Loss test: 2.067766 Accuracy train: 0.25823317307692306 Accuracy test: 0.2576\n",
            "Epoch 9 completed out of 20 loss: 2.03327270868497 Loss test: 2.0020564 Accuracy train: 0.29048477564102565 Accuracy test: 0.2862\n",
            "Epoch 10 completed out of 20 loss: 1.9750503870157095 Loss test: 1.9552895 Accuracy train: 0.31229967948717946 Accuracy test: 0.2983\n",
            "Epoch 11 completed out of 20 loss: 1.937430397975139 Loss test: 1.9283413 Accuracy train: 0.32427884615384617 Accuracy test: 0.311\n",
            "Epoch 12 completed out of 20 loss: 1.9116841725814038 Loss test: 1.90725 Accuracy train: 0.3341746794871795 Accuracy test: 0.3205\n",
            "Epoch 13 completed out of 20 loss: 1.8889373602011266 Loss test: 1.8859937 Accuracy train: 0.343349358974359 Accuracy test: 0.3289\n",
            "Epoch 14 completed out of 20 loss: 1.8669453880725764 Loss test: 1.8654372 Accuracy train: 0.35342548076923075 Accuracy test: 0.3372\n",
            "Epoch 15 completed out of 20 loss: 1.8452234179545672 Loss test: 1.8436704 Accuracy train: 0.36364182692307695 Accuracy test: 0.3462\n",
            "Epoch 16 completed out of 20 loss: 1.8232161827576465 Loss test: 1.8211254 Accuracy train: 0.3735576923076923 Accuracy test: 0.3549\n",
            "Epoch 17 completed out of 20 loss: 1.80061439428574 Loss test: 1.7975814 Accuracy train: 0.38407451923076924 Accuracy test: 0.362\n",
            "Epoch 18 completed out of 20 loss: 1.7776331577545557 Loss test: 1.7731746 Accuracy train: 0.39463141025641024 Accuracy test: 0.3705\n",
            "Epoch 19 completed out of 20 loss: 1.7550718683462876 Loss test: 1.7504971 Accuracy train: 0.405088141025641 Accuracy test: 0.3776\n",
            "El mejor accuracy en test es: 0.3776\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}